{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook performs all data preprocessing needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Defining parameters\n",
    "\n",
    "Here you define all parameter of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT_SIZE is the number of tokens that you will considerate to predict the next\n",
    "CONTEXT_SIZE = 5\n",
    "\n",
    "# VOCAB_LEN is the max unique tokens to be considered in the vocab dict\n",
    "VOCAB_LEN = 70000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Importing libs\n",
    "\n",
    "Here you will import all necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import *\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Reading raw dataset\n",
    "\n",
    "Here the raw dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"../deliver/train/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523356\n"
     ]
    }
   ],
   "source": [
    "# Printing some examples\n",
    "unique_tokens = ' '.join(data['text'].values).lower()\n",
    "train_tokens = unique_tokens.split()\n",
    "print(len(train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique tokens:  44895\n"
     ]
    }
   ],
   "source": [
    "# Printing how many unique tokens is there\n",
    "print('Length of unique tokens: ', len(set(train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['this', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n"
     ]
    }
   ],
   "source": [
    "print('First 10 tokens:',train_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode for EAP: 44897 MWS: 44898 HPL: 44899\n",
      "Vocab has 44900 tokens\n",
      "10 sample tokens: ['the', 'of', 'and', 'to', 'a', 'i', 'in', 'was', 'that', 'my']\n"
     ]
    }
   ],
   "source": [
    "# Effectively building our vocabulary\n",
    "vocab = build_vocab(train_tokens, vocab_size=VOCAB_LEN)\n",
    "\n",
    "# Adding the 3 author in vocab\n",
    "vocab['EAP'] = len(vocab) \n",
    "vocab['MWS'] = len(vocab)\n",
    "vocab['HPL'] = len(vocab)\n",
    "\n",
    "print(f\"Encode for EAP: {vocab['EAP']} MWS: {vocab['MWS']} HPL: {vocab['HPL']}\")\n",
    "\n",
    "print(f'Vocab has {len(vocab)} tokens')\n",
    "print(f'10 sample tokens: {list(itertools.islice(vocab.keys(), 10))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an auxiliary \"transposed\" vocabulary to generate words\n",
    "\n",
    "vocab_t = defaultdict(list)\n",
    "for k, v in vocab.items():\n",
    "    vocab_t[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting vocabulary to further usage\n",
    "with open('obj/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "with open('obj/vocab_t.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED TOKENS:\n",
      "\n",
      " ([[44896, 44896, 44896, 44896, 44896, 33093], [44896, 44896, 44896, 44896, 33093, 9], [44896, 44896, 44896, 33093, 9, 304], [44896, 44896, 33093, 9, 304, 23], [44896, 33093, 9, 304, 23, 44895], [33093, 9, 304, 23, 44895, 2], [9, 304, 23, 44895, 2, 5], [304, 23, 44895, 2, 5, 118], [23, 44895, 2, 5, 118, 803], [44895, 2, 5, 118, 803, 3], [2, 5, 118, 803, 3, 833], [5, 118, 803, 3, 833, 44895], [118, 803, 3, 833, 44895, 5], [803, 3, 833, 44895, 5, 423], [3, 833, 44895, 5, 423, 76], [833, 44895, 5, 423, 76, 113], [44895, 5, 423, 76, 113, 35]], [9, 304, 23, 44895, 2, 5, 118, 803, 3, 833, 44895, 5, 423, 76, 113, 35, 56])\n",
      "\n",
      "\n",
      "DECODED TOKENS:                                     TARGET TOKEN \n",
      "\n",
      "\n",
      "<pad> <pad> <pad> <pad> <pad> hey                      my\n",
      "<pad> <pad> <pad> <pad> hey my                      name\n",
      "<pad> <pad> <pad> hey my name                      is\n",
      "<pad> <pad> hey my name is                      <unk>\n",
      "<pad> hey my name is <unk>                      and\n",
      "hey my name is <unk> and                      i\n",
      "my name is <unk> and i                      am\n",
      "name is <unk> and i am                      pleasure\n",
      "is <unk> and i am pleasure                      to\n",
      "<unk> and i am pleasure to                      meet\n",
      "and i am pleasure to meet                      <unk>\n",
      "i am pleasure to meet <unk>                      i\n",
      "am pleasure to meet <unk> i                      really\n",
      "pleasure to meet <unk> i really                      like\n",
      "to meet <unk> i really like                      how\n",
      "meet <unk> i really like how                      you\n",
      "<unk> i really like how you                      are\n"
     ]
    }
   ],
   "source": [
    "# Example of usage of get_ngrams function\n",
    "example = 'hey my name is lucas and i am pleasure to meet you! i really like how you are'\n",
    "example = example.split()\n",
    "encoded_tokens = get_ngrams(example, vocab ,n = 6)\n",
    "\n",
    "print(\"ENCODED TOKENS:\\n\\n\", encoded_tokens)\n",
    "\n",
    "print(\"\\n\\nDECODED TOKENS:\", 35*' ', \"TARGET TOKEN\",\"\\n\\n\")\n",
    "for i,f in enumerate(encoded_tokens[0]):\n",
    "    print(tokens2word(list(f), vocab_t), 20*' ',tokens2word([encoded_tokens[1][i]], vocab_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Generating and exporting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, t, a = generate_df(data, vocab, vocab_t, len_context = CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523356"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[44896, 44896, 44896, 44896, 44897]</td>\n",
       "      <td>44895</td>\n",
       "      <td>44897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[44896, 44896, 44896, 44897, 44895]</td>\n",
       "      <td>8206</td>\n",
       "      <td>44897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[44896, 44896, 44897, 44895, 8206]</td>\n",
       "      <td>141</td>\n",
       "      <td>44897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[44896, 44897, 44895, 8206, 141]</td>\n",
       "      <td>1330</td>\n",
       "      <td>44897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[44897, 44895, 8206, 141, 1330]</td>\n",
       "      <td>30</td>\n",
       "      <td>44897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               context  target  author\n",
       "0  [44896, 44896, 44896, 44896, 44897]   44895   44897\n",
       "1  [44896, 44896, 44896, 44897, 44895]    8206   44897\n",
       "2   [44896, 44896, 44897, 44895, 8206]     141   44897\n",
       "3     [44896, 44897, 44895, 8206, 141]    1330   44897\n",
       "4      [44897, 44895, 8206, 141, 1330]      30   44897"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'context': l, 'target': t, 'author': a})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting in train and validation dataset\n",
    "df_train, df_val = train_test_split(df, test_size = 0.1, random_state = 42, shuffle= True, stratify=df.author)\n",
    "\n",
    "df_val_authors = df_val.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting as .csv to further usage\n",
    "\n",
    "df_train.to_csv(\"../data/df_train.csv\", index = False)\n",
    "df_val.to_csv(\"../data/df_val.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must have \"df_train.csv\" and \"df_val.csv\" files in your \"data\" folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reprod",
   "language": "python",
   "name": "reprod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
