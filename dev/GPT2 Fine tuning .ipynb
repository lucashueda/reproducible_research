{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn. model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\"\n",
    "else: \n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setando seed para replicabilidade\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi my name is lucas'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando tokenizador (Byte level)\n",
    "\n",
    "tokenizer.decode(tokenizer.encode('hi my name is lucas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5303,   616,  1438,   318, 17115,   292]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hi my name is lucas',  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, out = model(tokenizer.encode('hi my name is lucas',  return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 12, 6, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time, the only other thing that mattered was the need to help him save his father, who had been in such a bad way, after an incident of his own. A friend's dream, in which she was being held captive by\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50, top_p=.95, top_k=50, do_sample= True, num_return_sequences=1)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../deliver/train/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5303, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus('hi', max_length= 10, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que cria n-grams para transformar nossos tokens em entrada para a rede baseado em https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "def get_ngrams(tokens, vocab ,n = 5):\n",
    "    '''\n",
    "    Função que recebe uma lista de tokens e retorna os índices, de acordo com o vocab, dos n-grams de tamanho n com base no vocab passado\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Primeiro adicionamos n-1 <pad> no começo dos tokens\n",
    "    local_tokens = (n-1)*['<pad>'] + tokens.copy() \n",
    "\n",
    "    # Array que guardará o vetor de contextos\n",
    "    context_df = []\n",
    "    # Array que guardará os tokens target para cada contexto\n",
    "    target_token = []\n",
    "\n",
    "  # Para cada token de 0 a len(tokens) - n - 1(já que vamos usar até i+n tokens por contexto e o i+n+1 é o target)\n",
    "    for i in range(len(local_tokens) - n):\n",
    "    # Vetor auxiliar que será incrementado ao context_df\n",
    "        aux_df = []\n",
    "\n",
    "        # Loop que percorre os primeiros n tokens\n",
    "        for j in range(i, i+n):\n",
    "            if(local_tokens[j] not in vocab):\n",
    "                aux_df.append(vocab[\"<unk>\"])\n",
    "            else:\n",
    "                aux_df.append(vocab[local_tokens[j]])\n",
    "\n",
    "        # Incrementa o context_df\n",
    "        context_df.append(aux_df)\n",
    "\n",
    "        # Incrementa o target_token com o i+n+1 token\n",
    "        if(local_tokens[i+n] not in vocab):\n",
    "            target_token.append(vocab[\"<unk>\"])\n",
    "        else:\n",
    "            target_token.append(vocab[local_tokens[i+n]])\n",
    "\n",
    "    # Retorno numpy arrays por comodidade minha, mas poderiam ser tensores direto\n",
    "    return context_df, target_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(df, len_context = 5):\n",
    "    results_list = []\n",
    "    target_list = []\n",
    "    author_list = []\n",
    "    \n",
    "    i = 0\n",
    "    for text, author in zip(df.text, df.author):\n",
    "#         print([text],author)\n",
    "        encoded = get_ngrams(text.split(), vocab ,n = len_context)\n",
    "#         print([text],encoded[0])\n",
    "#         i+=1\n",
    "#         if(i>10):\n",
    "#             break\n",
    "        \n",
    "        results_list.extend(encoded[0])\n",
    "        target_list.extend(encoded[1])\n",
    "        author_list.extend(len(encoded[1])*[author])\n",
    "        \n",
    "    return results_list, target_list, author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
