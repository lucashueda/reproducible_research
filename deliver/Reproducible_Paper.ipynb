{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controllable spooky text generation based on author's embedding\n",
    "\n",
    "### Lucas H. Ueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Abstract—Natural language processing (NLP) has advanced\n",
    "very recently. With the emergence of Transformers and networks\n",
    "based on them, textual generation has shown itself to be\n",
    "increasingly closer to human, as the texts generated by GPT-2 and\n",
    "more recently GPT-3. However, in these architectures the control\n",
    "of the generated text style is not considered. This project seeks,\n",
    "based on a language model approach, to study the possibility\n",
    "of controlling the writing style based on the differentiation of\n",
    "authors in a latent space (author embedding) generated by the\n",
    "model. In particular we work with 3 horror story writers (Marry\n",
    "Shelley, Edgar Allan Poe and H.P. Lovecraft). Our source code is\n",
    "publicly available at https://github.com/lucashueda/reproducible\n",
    "research.***\n",
    "\n",
    "***Index Terms—NLP, Language modeling, embedding,\n",
    "neural networks, controllable***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this to do the data preprocess (almost in an instant )\n",
    "%%capture\n",
    "%cd ../dev\n",
    "%run ../dev/1_data_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this to do the model training (almost 20min in a RTX 2060 6gb)\n",
    "%%capture\n",
    "import time\n",
    "init = time.time()\n",
    "%run ../dev/2_training_language_model.ipynb\n",
    "end = time.time()\n",
    "print(end - init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371.2590572834015\n"
     ]
    }
   ],
   "source": [
    "# Just print the time to run training cell\n",
    "print(end - init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "LANGUAGE modelling tries to estimate a probability\n",
    "density function that can predict next token by the past\n",
    "ones [1]. This technique allows us to represent words in a\n",
    "latent space by using dense vectors with fixed dimensional.\n",
    "This dense vector is used in the literature with two main\n",
    "purposes, the first is a better representation of a phrase instead\n",
    "of an one hot vector and the second one is to control latent\n",
    "meanings by finding patterns in this latent space ([2], [3]).\n",
    "\n",
    "Text generation it’s a very hard task in NLP because of its\n",
    "”human” nature, it is very hard to find a way to generate text\n",
    "as a human since there is no specific pattern known in humans\n",
    "generated texts. Text DE-Generation is yet a big problem\n",
    "which makes algorithms very repetitive and with no reasonable\n",
    "meaning of the generated texts [4]. Additionally there is no\n",
    "main way to validate a text generator in terms of reasonable\n",
    "and coherence with human language in an automatic way.\n",
    "Additionally to control the text that is generated is one more\n",
    "problem in this task.\n",
    "\n",
    "In this work we try to make a fully text generator system\n",
    "that is able to produce reasonable texts conditioned by an\n",
    "author embedding, i.e., by given as input an initial text and\n",
    "a author embedding the system is able to produce text as\n",
    "this target author. We use an dataframe that has texts from\n",
    "3 spooky authors, Edgar Allan Poe, H.P. Lovecraft and Marry\n",
    "Shelley. This is a dataframe from a kaggle competition and\n",
    "has a lot of sentences of these 3 authors, we will use this\n",
    "dataframe to extract author’s embeddings and generate our\n",
    "proposed method.\n",
    "\n",
    "This work is organized as follows: Section 1 is the\n",
    "introduction, Section 2 we describe the methods that motives\n",
    "our proposed method, in Section 3 we describe the experiments\n",
    "made in the dataset and the architectures tested, Section 4 and 5 we discuss and conclude our work. Further sections are the\n",
    "source code and acknowledgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD\n",
    "\n",
    "In this section we are going to summarize the motivations\n",
    "of our proposed method.\n",
    "\n",
    "### A. LANGUAGE MODELLING\n",
    "\n",
    "Language modelling is the process to estimate the\n",
    "probability density function that can predict a token given an\n",
    "array of past tokens [1]. This approach can be used to generate\n",
    "tokens, but the more gain about this technique was about word\n",
    "representation [2] where a dense vector could represent better\n",
    "a token than its one-hot vector, it allows a lot of evolution in\n",
    "NLP with GloVE, ElMO and more recently with Transformer.\n",
    "\n",
    "In this project we are going to use the meaning of\n",
    "language modelling to generate our tokens, we could use\n",
    "a sequence-to-sequence modelling instead of per token\n",
    "generation but for isolated effects purposes we choose to work\n",
    "with a simple way to do the token generation. Also we will\n",
    "use top-k and nucleus sampling [4] as decoding metodologies\n",
    "that are demonstrating to be the best for our purpose.\n",
    "\n",
    "### B. tSNE\n",
    "\n",
    "t-Distributed Stochastic Neighbot Embedding (tSNE) [5]\n",
    "is a non-linear technique for dimnesionality reduction that\n",
    "is particularly well suited for the data visualization of\n",
    "high-dimensional dataset. It is a technique commonly used\n",
    "in unstructured data, such as images, texts and speech.\n",
    "\n",
    "The tSNE is based on maintaining the similarity in the\n",
    "distribution of vectors that exist in the high dimension also\n",
    "in the low dimension. This makes it possible to maintain\n",
    "statistical characteristics of similarity of the data even in the\n",
    "smallest dimension. For the specific project it will be used to\n",
    "visualize the quality of the authors’ differentiation in the latent\n",
    "space generated by the model.\n",
    "\n",
    "### C. EVALUATION\n",
    "\n",
    "In order to measure the quality of our model we analyze\n",
    "classical loss curve between training and validation steps.\n",
    "We also look for Perplexity [6] which measure how well\n",
    "distributed are our logits compared to the correct token.\n",
    "Perplexity if often used to measure quality of next token\n",
    "generation task and thats why it will be chosen.\n",
    "\n",
    "In the future we will seek to evaluate the task from other\n",
    "metrics, such as BLEU [6] and MRR [7], as well as a\n",
    "customized authorial metric in order to measure the rate\n",
    "of words generated by the model in relation to an author\n",
    "compared to the most frequent words used by that author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS\n",
    "\n",
    "The dataset is balanced between the 3 author’s and have\n",
    "around 20.000 observations, each one with a sentence and a\n",
    "target of each author writes it. This dataset was preprocessed\n",
    "to be in the language modeling format. We perform 5 tokens\n",
    "of context size and the 6th as the target one, this give to us\n",
    "around 500.000 lines of observations which we use 90% as\n",
    "training dataset and 10% as validation one.\n",
    "\n",
    "The 2 models were performed using this 5 tokens context\n",
    "input tokens with a batch size of 256. All experiments were\n",
    "made in a local machine with 16gb RAM and a RTX 2060\n",
    "6gb GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. VANILLA MODEL\n",
    "\n",
    "As a baseline we use a Vanilla model that consists of the\n",
    "simplest implementation of the language model mentioned in\n",
    "Bengio’s, with a look up embedding layer and a linear layer\n",
    "that takes the dimension of embedding into the space of the\n",
    "vocabulary tokens.\n",
    "\n",
    "The model had as parameters an embedding dimension of\n",
    "128, with the SGD optimizer, a learning rate of 5e-3 and\n",
    "was run for 20 epochs. It is possible to see how difficult it\n",
    "is for a simple model to achieve good perplexity, as shown in\n",
    "the Figure 1. Additionally, the visualization in 2 dimensions\n",
    "(Figure 2) shows that the model cannot differentiate the\n",
    "authors, indicating that it lacks the capacity to generate a\n",
    "differentiable latent space in relation to the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../dev/partial_results/vanilla_model_train_perplexity_graph.png\" width=\"1000\">\n",
    "\n",
    "Fig 1: Loss and perplexity of the Vanilla model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../dev/partial_results/vanilla_model_umap_embedding.png\" width=\"1000\">\n",
    "Fig 2: Author's embeddings of the Vanlla model by tSNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. PROPOSED MODEL\n",
    "\n",
    "Our proposed model consists of adding a recurrent unit\n",
    "GRU [8] to the end of the loop up embedding layer of the\n",
    "Vanilla model, here the model was also run for 20 epochs\n",
    "with a learning rate of 5e-3 in a SGD optimizer. The recurrent\n",
    "layer aims at a temporal analysis of the generated tokens, thus\n",
    "allowing greater assertiveness of the model and consequently\n",
    "the generation of a differentiable latent space.\n",
    "\n",
    "Unfortunately, the model also failed to converge well for\n",
    "the dataset in question (Figure 3). However, it is possible to observe that it was able to generate a more differentiable space\n",
    "than the previous one (Figure 4). Differentiation was what\n",
    "was sought and therefore it is expected that a more robust\n",
    "architecture allows a more assertive space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../dev/partial_results/proposed_model_train_perplexity_graph.png\" width=\"1000\">\n",
    "Fig 3: Loss and perplexity of the Proposed model.\n",
    "\n",
    "<img src=\"../dev/partial_results/proposed_model_umap_embedding.png\" width=\"1000\">\n",
    "Fig 4: Author's embeddings of the Proposed model by tSNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in the future is that a model structure based\n",
    "on Transformers [9] be developed, as it has been showing\n",
    "great results in NLP tasks. For the specific task of generating\n",
    "latent space, we will try to use the technique of Variational\n",
    "AutoEncoder (VAE) [3] where it is possible to generate a latent\n",
    "Gaussian space from the optimization of a probability density\n",
    "function that describes the generator of the dense vectors of\n",
    "each author. Finally, at the time of inference, the centroid of\n",
    "each grouping of authors in the latent space will be used as a\n",
    "parameter for the control of textual generation, the final model\n",
    "proposal is outlined in the Figure 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/proposed_model.png\" width=\"1000\">\n",
    "Fig 5: Pipeline of training of the future aimed model. In inference\n",
    "time there is no connection between embedding layer and VAE\n",
    "encoder layer, the target author will be setted by the hard coded\n",
    "centroid of author in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCUSSION\n",
    "\n",
    "The results show that models like states are not able\n",
    "to effectively model the probability density function that\n",
    "generates the tokens of the dataset in question. The low loss\n",
    "in validation and perplexity value show an easy overfittig of\n",
    "the model.\n",
    "\n",
    "The proposed model can minimally differentiate the authors\n",
    "in the latent space in a smaller dimension, while the vanilla\n",
    "model cannot differentiate any of the actors. The quality of\n",
    "the models as language models preclude a deeper analysis\n",
    "of their quality for the task of controlling textual generation,\n",
    "however it is reasonable to assume that it is necessary to use\n",
    "more robust and more consolidated architectures in the current\n",
    "literature. The proposed model for the future looks promising\n",
    "given works in similar areas that have achieved good results\n",
    "using such techniques [10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "Textual generation is definitely a difficult task. Adding\n",
    "control to this generation proved to be a far more complicated\n",
    "task than it was imagined to accomplish in time for a\n",
    "discipline. Tests with state-of-the-art models were carried out,\n",
    "however without success in controlling textual generation.\n",
    "However, it is hoped that in the future with a model based\n",
    "on Transformers it will be possible to differentiate the latent\n",
    "space more, in addition to a better performance in the task of\n",
    "language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE CODE\n",
    "\n",
    "All the code used to do this project is available in https:\n",
    "//github.com/lucashueda/reproducible research.\n",
    "\n",
    "## REFERENCES\n",
    "\n",
    "[1] P. V. C. J. Yoshua Bengio, Rejean Ducharme, “A neural probabilistic ´\n",
    "language model,” online: http://www.jmlr.org/ papers/volume3/\n",
    "bengio03a/ bengio03a.pdf , 2003.\n",
    "\n",
    "[2] G. C. J. D. Tomas Mikolov, Kai Chen, “Efficient estimation of word\n",
    "representations in vector space,” online: https:// arxiv.org/ abs/ 1301.\n",
    "3781, 2003.\n",
    "\n",
    "[3] M. W. Diederik P Kingma, “Auto-encoding variational bayes,” online:\n",
    "https:// arxiv.org/ abs/ 1312.6114, 2013.\n",
    "\n",
    "[4] L. D. M. F. Y. C. Ari Holtzman, Jan Buys, “The curious case of neural\n",
    "text degeneration,” online: https:// arxiv.org/ abs/ 1904.09751, 2019.\n",
    "\n",
    "[5] G. H. Laurens van der Maaten, “Visualizing data using t-sne,”\n",
    "online: http://www.jmlr.org/ papers/volume9/vandermaaten08a/\n",
    "vandermaaten08a.pdf , 2008.\n",
    "\n",
    "[6] R. R. Stanley Chen, Douglas Beeferman, “Evaluation metrics\n",
    "for language models,” online: https://www.cs.cmu.edu/∼roni/ papers/\n",
    "eval-metrics-bntuw-9802.pdf , 2001.\n",
    "\n",
    "[7] N. Craswell, Mean Reciprocal Rank, pp. 1703–1703. Boston, MA:\n",
    "Springer US, 2009.\n",
    "\n",
    "[8] K. C. Y. B. Junyoung Chung, Caglar Gulcehre, “Empirical evaluation of\n",
    "gated recurrent neural networks on sequence modeling,” online: https:\n",
    "// arxiv.org/ abs/ 1412.3555, 2014.\n",
    "\n",
    "[9] N. P. J. U. L. J. A. N. G. L. K. I. P. Ashish Vaswani, Noam Shazeer,\n",
    "“Attention is all you need,” online: https:// arxiv.org/ abs/ 1706.03762,\n",
    "2017.\n",
    "\n",
    "[10] Y. M. Kei Akuzawa, Yusuke Iwasawa, “Expressive speech synthesis\n",
    "via modeling expressions with variational autoencoder,” online: https:\n",
    "// arxiv.org/ abs/ 1804.02135, 2018.\n",
    "\n",
    "## ACKNOWLEDGEMENTS\n",
    "\n",
    "This project is part of Computational Reproducible Research\n",
    "course at Unicamp (1S2020)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reprod",
   "language": "python",
   "name": "reprod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
